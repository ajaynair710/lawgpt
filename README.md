# ðŸ“œ Law-GPT: Legal Language Model for Indian Penal Code (IPC)

**Law-GPT** is a lightweight, transformer-based language model fine-tuned to assist with queries about the Indian Penal Code (IPC). It can:

* Explain IPC sections in simple language.
* Summarize sections.
* Suggest relevant IPC sections for legal cases.

This project demonstrates **LLM fine-tuning (LoRA)**, **RAG pipelines**, and **end-to-end evaluation** â€” optimized to run on local hardware.

---

## ðŸ”¹ Project Structure

```
law-gpt-ipc/
â”‚â”€â”€ data/
â”‚   â”œâ”€â”€ ipc.pdf                 # Official IPC PDF
â”‚   â”œâ”€â”€ ipc_parser.py           # Extract sections from PDF
â”‚   â”œâ”€â”€ ipc_sections.json       # Structured IPC sections
â”‚   â”œâ”€â”€ ipc_instructions.py     # Instruction dataset generator
â”‚   â”œâ”€â”€ ipc_instructions.json   # Instruction dataset
â”‚   â””â”€â”€ eval_set.json           # Small evaluation set
â”‚
â”‚â”€â”€ rag/
â”‚   â”œâ”€â”€ rag_pipeline.py         # RAG inference pipeline
â”‚
â”‚â”€â”€ train/
â”‚   â”œâ”€â”€ finetune_lora.py # Fine-tune LoRA on small model
â”‚
â”‚â”€â”€ eval/
â”‚   â”œâ”€â”€ evaluate_models.py       # Evaluate Base, RAG, Fine-tuned
â”‚
â”‚â”€â”€ app/
â”‚   â”œâ”€â”€ app.py                   # Gradio demo app
â”‚
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md
```

---

## ðŸ”¹ Installation

1. Clone the repo:

```bash
git clone https://github.com/ajaynair710/lawgpt.git
cd lawgpt
```

2. Install dependencies:

```bash
pip install -r requirements.txt
```

---

## ðŸ”¹ Dataset Preparation

1. Place official IPC PDF in `data/ipc.pdf`.
2. Extract structured sections:

```bash
python data/ipc_parser.py
```

3. Generate instruction dataset (\~2k examples):

```bash
python data/ipc_instructions.py
```

---

## ðŸ”¹ RAG Pipeline

Build embeddings and perform retrieval + generation:

```bash
python rag/rag_pipeline.py
```

Example query:

```text
A person forcibly entered someone's house at night with intent to harm. Which IPC sections may apply?
```

---

## ðŸ”¹ Fine-Tuning

Fine-tune a **small GPT model (distilgpt2)** using LoRA:

```bash
python train/train_finetune_small.py
```

* Adapter weights saved in `lawgpt-ipc-lora/`
* Small model = fast local training (<30 min CPU, <10 min GPU). 
* I have done CPU training on my local.

---

## ðŸ”¹ Evaluation

Evaluate **Base**, **RAG**, and **Fine-Tuned** models:

```bash
python eval/evaluate_models.py
```

Metrics computed:

* **ROUGE-L** â†’ for summarization/explanation
* **F1** â†’ for suggested sections
* **Grounding %** â†’ proportion of outputs mentioning valid section numbers

Example results (small model):

```
Base       | ROUGE-L: 0.168 | F1: 0.000 | Grounding: 80.0%
RAG        | ROUGE-L: 0.030 | F1: 0.165 | Grounding: 100.0%
Fine-tuned | ROUGE-L: 0.059 | F1: 0.000 | Grounding: 80.0%
```

> Note: Small model used for local demo. Larger models (Mistral-7B) will significantly improve performance.

---

## ðŸ”¹ Demo App

Launch a Gradio demo:

```bash
python app/app.py
```

* Input your query and choose:

  * Base
  * RAG
  * Fine-tuned
* Outputs include **cited IPC sections**.
* Can be deployed to **Hugging Face Spaces**.

---

## ðŸ”¹ Usage Examples

```python
from rag.rag_pipeline import rag_query

query = "A person committed theft and trespassed a house."
answer = rag_query(query)
print(answer)
```

```python
# Fine-tuned inference
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_dir = "lawgpt-ipc-lora"
tokenizer = AutoTokenizer.from_pretrained(model_dir)
model = AutoModelForCausalLM.from_pretrained(model_dir).to("cuda" if torch.cuda.is_available() else "cpu")

prompt = "Explain IPC Section 420"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

---

## ðŸ”¹ Key Features

* **Instruction Fine-Tuning (LoRA)** â†’ small footprint, fast training.
* **RAG Pipeline** â†’ retrieval + generation improves grounding.
* **Evaluation Suite** â†’ ROUGE, F1, grounding metrics.
* **Demo Web App** â†’ interactive query interface.

---

## ðŸ”¹ Future Improvements

* Use **larger LLMs** (Mistral/Falcon) for better quality.
* Expand instruction dataset to 10k+ examples.
* Add **multi-turn legal QA** support.
* Combine RAG + fine-tuned model for **hybrid approach**.

---

## ðŸ”¹ Disclaimer

**Law-GPT is for educational/demo purposes only. It does not provide legal advice. Users should consult qualified legal professionals.**

---

